«««
code: CS274A
name: Natural Language Processing
semester: Spring 2023
category: Homework 作业
title: Homework 4
»»»

# Homework 4

## Page 1 (question)

@ Problem - checkbox

title: "Question 1 - Beyond CFG"
content: """
Select all correct statements
"""
choice: """
There are no polynomial-time parsing algorithms for Mildly context-sensitive grammars.
Mildly context-sensitive grammars are more expressive than PCFGs.
In span-based parsing, we need to compute scores for every possible span, and use the CKY algorithm to find the highest-scoring tree. 
Constituency parsing can be casted to a sequence-labeling task. The main advantage is the faster parsing speed. 
"""
points: "10"
answer: "BCD"


@ Problem - text

title: "Question 2 - Constituency parsing evaluation"
content: """
(Note: indexes here are left inclusive and right exclusive)

Gold standard brackets are:  S-(0:11), NP-(0:2), VP-(2:9), VP-(3:9), NP-(4:6), PP- (6-9), NP-(7,9), NP-(9:10). 

and that of predicted parse tree are: S-(0:11), NP-(0:2), VP-(2:10), VP-(3:10), NP-(4:6), PP-(6-10), NP-(7,10). 

What is the F1 score?  (Answer should be  XX.XX %, you need to enter XX.XX as your answer, excluding %)
"""
points: "10"
answer: "40"


@ Problem - checkbox

title: "Question 3 - PCFG"
content: """
Select all correct statements.
"""
choice: """
The inside algorithm is fully differentiable, thus we can use automatic differentiation to backpropagate through the inside algorithm. 
The time complexity of the inside algorithm is $O(n^3 |G|)$, where $n$ is the sentence length and $|G|$ is the grammar size.
Any CFGs can be rewritten into Choskey Normal form. The resulting parse tree can be non-binary. 
The CKY algorithm can be applied to any CFGs for parsing.
"""
points: "10"
answer: "AB"


@ Problem - checkbox

title: "Question 4 - PCFG"
content: """
Select all correct statements.
"""
choice: """
Weighted context-free grammar (WCFG) has the same expressiveness as probabilistic context-free grammar (PCFG). 
We can use the maximum likelihood estimator (MLE) to learn a PCFG on a treebank. MLE often leads to good performance because PCFGs are very expressive.
Latent-variable PCFG split each nonterminal into a finite number of subtypes, regarding nonterminal subtypes in training parse trees as latent variables. 
We can use the EM algorithm for unsupervised learning of PCFGs, and use the inside-outside algorithm to compute expected counts. 
"""
points: "10"
answer: "ACD"


@ Problem - checkbox

title: "Question 5 - Constituency Parsing 1"
content: """
Assume we have the following context-free grammar G in Chomsky normal form:

<img src="https://s1.ax1x.com/2022/06/13/XRToWR.jpg" width="50%"/>

Which of the following trees for the sentence "workers dumped sacks of garbage and junk into a bin" are correct according to G?
"""
choice: """
<img src='https://s1.ax1x.com/2022/06/13/XR7Klq.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7QXV.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7YtJ.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7BnK.jpg' width='50%'/>
"""
points: "10"
answer: "AC"


@ Problem - checkbox

title: "Question 6 - Constituency Parsing 2"
content: """
Which of the following is/are true about the CKY algorithm?
"""
choice: """
CKY is a top-down parsing algorithm
It requires the grammar be in Chomskey Normal Form (CNF)
CKY runs in cubic time with respect to the sentence length
CKY can be converted to the inside algorithm by replacing <b>max</b> with <b>sum</b>.
"""
points: "10"
answer: "BCD"


@ Problem - checkbox

title: "Question 7 - Constituency Parsing 3"
content: """
Consider the language defined by the following grammar.

<img src="https://s1.ax1x.com/2022/06/13/XRHsK0.md.jpg" width="50%"/>

Select all sentences that are generated by the grammar.
"""
choice: """
oslo in snow adores kim
kim adores snow in oslo
snow adores in oslo kim
in kim adores oslo snow
"""
points: "10"
answer: "AB"


@ Problem - radio

title: "Question 8 - Constituency Parsing 4"
content: """
Consider the sentence "I like to run" and a PCFG with nonterminals {S, VP, N, P, V}, terminals {I, like, to, run}, start symbol S, and rules

<center><img src="https://s1.ax1x.com/2022/06/13/XRbPZ8.jpg" width="20%"/></center>

<center><img src="https://s1.ax1x.com/2022/06/13/XRbVRs.jpg" width="30%"/></center>

The figure is a CKY chart, with possible symbols for the leaves already filled in. We want to fill the chart with nonterminals with log probability greater than negative infinity (i.e., nonterminals that are possible to build over these spans of the sentence). Which other cells contain at least one nonterminal with log probability greater than negative infinity?
"""
choice: """
1, 3, 6
1, 3, 4, 6
1, 2, 3, 4, 6
1, 3, 4, 5, 6
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 9 - Constituency Parsing 5"
content: """
Consider the following grammars. They all share the same lexicon.

<img src="https://s1.ax1x.com/2022/06/13/XRbqO0.md.jpg" width="50%"/>

Which of these grammars is in Chomsky Normal Form?
"""
choice: """
Grammar 1
Grammar 2
Grammar 3
None of the above.
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 10 - Span-based Constituency Parsing"
content: """
Given a span-based constituency parsing model. Select all correct statement(s):
"""
choice: """
If the goal of the model is to find the parse tree $t* = \text{argmax}_{t \in T_x} S(t|x)$ of a given sentence x, the model is a generative model.
In Slide10's span-based constituency parsing model, we employ the score of the most probable label for each span to execute the CYK algorithm.
We can use some strong word representations such as BERT to enhance the span-based constituency parsing model
"""
points: "10"
answer: "BC"