«««
code: CS274A
name: Natural Language Processing
semester: Spring 2023
category: Homework 作业
title: Homework 1
»»»

# Homework 1

## Page 1 (question)


@ Problem - radio

title: "Question 1 - Regular expression"
content: """
Which regular expression can match simple English contractions "XXXn't"?

Please ignore cases of letters and all punctuations except the single quotation mark "'".

The "\b" in choices represents the word boundaries, which is a special token in RE like "\s" and "\w".

Examples:  

|Input|Output|Explanation|
|:--|:--|:--|
|I don't know|don't| |
|I can't|can't| |
|I ca n't|- (no output)|n't has no preceeding XXX.|
|I can'tdo|- (no output)|n't is not the end of a word.|
|I c9n't|- (no output)|an English word does not contain digits.|

The outputs are obtained by the python statement: `re.search(r"pattern", "testcase", flags=re.IGNORECASE)`. Do not forget the `r` before the pattern string.
"""
choice: """
\b\w*n't\b
\b[a-z]*n't\b
\b[a-z]+n't\b
\b[a-z]|[a-z]*n't\b
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 2 - BPE Tokenization"
content: """
Assume a BPE tokenizer with vocab `{_, t, o, g, e, h, r, he, the, er, r_, er_, to, ge, get}` in the learned order, `together` will be tokenized into
"""
choice: """
`t o g e t h e r _`
`to get he r_`
`to ge the r_`
`to ge th er_`
"""
points: "10"
answer: "C"


@ Problem - custom

title: "Question 3 - Word Normalization"
content: """
Consider the sentence: Kitty's cat was found behind the television.
Choose the technique corresponding to the processed result:
"""
choice: """
<div class="question q1 q-radio" answer="{'A': 3.3}">
    <p>Kitti cat wa found behind the televis</p>
    <input type="radio" name="q1" value="A" title="stemming">
    <input type="radio" name="q1" value="B" title="lemmatization">
    <input type="radio" name="q1" value="C" title="case folding">
    <p class="tick-label"></p>
</div>
<div class="question q2 q-radio" answer="{'B': 3.3}">
    <p>Kitty 's cat be find behind the television</p>
    <input type="radio" name="q2" value="A" title="stemming">
    <input type="radio" name="q2" value="B" title="lemmatization">
    <input type="radio" name="q2" value="C" title="case folding">
    <p class="tick-label"></p>
</div>
<div class="question q3 q-radio" answer="{'C': 3.4}">
    <p>kitty’s cat was found behind the television</p>
    <input type="radio" name="q3" value="A" title="stemming">
    <input type="radio" name="q3" value="B" title="lemmatization">
    <input type="radio" name="q3" value="C" title="case folding">
    <p class="tick-label"></p>
</div>
"""


## Page 2 (question)

@ Problem - checkbox

title: "Question 4 - Word Representation 1"
content: """
Choose all of the word vector models that produce **static** and **dense** word embeddings.
"""
choice: """
One-hot vectors
PPMI vectors (without SVD)
BERT
Word2vec skip-grams
Latent Semantic Analysis (LSA)
"""
points: "10"
answer: "DE"


@ Problem - checkbox

title: "Question 5 - Sparse and dense word representations"
content: """
Select all correct statements.
"""
choice: """
One-hot word vectors are unable to capture word similarity.
A word-word co-occurrence matrix captures some word similarity.
PPMI matrix is sparse, but it can produce dense word embeddings after applying SVD decomposition.
Word-word PPMI matrix, one-hot vectors , and Word2vec assign a fixed embedding for each word independent of contexts.
"""
points: "10"
answer: "ABCD"


@ Problem - custom

title: "Question 6 - PPMI matrix"
content: """
Given a term-context (word-word) co-occurrence matrix:

puppy and panda are words, (pet, cute, china) are contexts

|co-occurrence|pet|cute|china|
|:--:|:--:|:--:|:--:|
|puppy|5|10|0|
|panda|0|15|20|

"""
choice: """
<div class="question q1 q-text" answer="{'50': 2, '50.00': 2}">
    <p style="display: inline;">Calculate the number of all occurrences: </p>
    <input type="text" name="q1" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q2 q-text" answer="{'0.4': 2, '0.40': 2}">
    <p style="display: inline;">Calculate p(word=panda, context=china): </p>
    <input type="text" name="q2" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q3 q-text" answer="{'0.51': 2}">
    <p style="display: inline;">Calculate the PMI(panda, china). use log2, not weighted, and use two decimal places,  (e.g., 0.37). </p>
    <input type="text" name="q3" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q4 q-text" answer="{'0': 2, '0.00': 2}">
    <p style="display: inline;">Calculate the PPMI(panda, cute). use log2, not weighted, and use two decimal places,  (e.g., 0.37). </p>
    <input type="text" name="q4" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q5 q-text" answer="{'0.51': 2}">
    <p style="display: inline;">Calculate the PPMI(panda, china). use log2, not weighted, and use two decimal places,  (e.g., 0.37). </p>
    <input type="text" name="q5" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q6 q-text" answer="{'0.47': 2}">
    <p style="display: inline;">Calculate the add-2 smoothed PPMI(panda, china). use log2, not weighted, and use two decimal places. </p>
    <input type="text" name="q6" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
"""

## Page 3 (question)


@ Problem - custom

title: "Question 7 - Train naive bayes"
content: """
Train a multinomial naive Bayes with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.

Note: the vocabulary consists of three words: good, poor and great. All other words are ignored.

|doc|"good"|"poor"|"great"|(class)|
|:--|:--|:--|:--|:--|
|d1.|3|0|3|pos|
|d2.|0|1|2|pos|
|d3.|1|3|0|neg|
|d4.|1|5|2|neg|
|d5.|0|2|0|neg|

Use both naive Bayes models to analysis sentence:

```
A good, good plot and great characters, but poor acting.
```

Recall that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don't throw out words that appear in some classes but not others; that's what add-1 smoothing is for.) 

The score(prior*likelihood) that the multinomial naive Bayes assign it to class positive is [Q1].

The score that the multinomial naive Bayes assign it to class negative is [Q2].

All answers should be irreducible fractions, e.g., 1/2.
"""
choice: """
<div class="question q1 q-text-no-eval" answer="{'1/270': 5}">
    <p style="display: inline;">Q1: </p>
    <input type="text" name="q1" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q2 q-text-no-eval" answer="{'891/417605': 5}">
    <p style="display: inline;">Q2: </p>
    <input type="text" name="q2" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
"""

## Page 4 (question)

@ Problem - custom

title: "Question 8 - The Confusion Matrix 1"
content: """
Consider the following confusion matrix:

|                 | gold positive | gold negative |
| :-------------: | :-----------: | :-----------: |
| system positive |      102      |       9       |
| system negative |      54       |      140      |

In this question, please use decimals to represent the final result and keep 3 decimal points (e.g. -0.500, 0.167).
"""
choice: """
<div class="question q1 q-text-no-eval" answer="{'0.919': 2.5}">
    <p style="display: inline;">What is the precision?</p>
    <input type="text" name="q1" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q2 q-text-no-eval" answer="{'0.654': 2.5}">
    <p style="display: inline;">What is the recall?</p>
    <input type="text" name="q2" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q3 q-text-no-eval" answer="{'0.793': 2.5}">
    <p style="display: inline;">What is the accuracy?</p>
    <input type="text" name="q3" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q4 q-text-no-eval" answer="{'0.764': 2.5}">
    <p style="display: inline;">What is the f1-score?</p>
    <input type="text" name="q4" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
"""


@ Problem - custom

title: "Question 9 - The Confusion Matrix 2"
content: """
Consider the following confusion matrix:

|              | gold tag 1 | gold tag 2 | gold tag 3 |
| :----------: | :--------: | :--------: | :--------: |
| system tag 1 |    42     |     7      |     11     |
| system tag 2 |     24     |    140     |     3      |
| system tag 3 |     9      |     16     |    132     |

In this question, please use decimals to represent the final result and keep 3 decimal points (e.g. -0.500, 0.167).
"""
choice: """
<div class="question q1 q-text-no-eval" answer="{'0.781': 5}">
    <p style="display: inline;">What is the macro-average f1-score?</p>
    <input type="text" name="q1" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
<div class="question q2 q-text-no-eval" answer="{'0.812': 5}">
    <p style="display: inline;">What is the micro-average f1-score?</p>
    <input type="text" name="q2" placeholder="" class="layui-input" style="display: inline;">
    <p class="tick-label" style="display: inline;"></p>
</div>
"""

## Page 5 (question)

@ Problem - radio

title: "Question 10 - Sigmoid and softmax"
content: """
Sigmoid is a generalization of Softmax.
"""
choice: """
Correct
Incorrect
"""
points: "10"
answer: "B"

@ Problem - checkbox

title: "Question 11 - Distance measure"
content: """
Which of the following clustering algorithms require(s) a distance measure?
"""
choice: """
K-means
Hierarchical Agglomerative Clustering (HAC)
Expectation-maximization with Mixture of Gaussian (MoG)
"""
points: "10"
answer: "AB"


@ Problem - radio

title: "Question 10 - Expectation Maximization 1"
content: """
In Expectation Maximization (EM) algorithm, we revise each cluster model based on its (proportionately) assigned points in E step and assign data instances proportionately to different models in M step.
"""
choice: """
Correct
Incorrect
"""
points: "10"
answer: "B"


@ Problem - radio

title: "Question 11 - Expectation Maximization 2"
content: """
The Expectation Maximization (EM) algorithm is an optimization algorithm. It will converge to a local optimum, but not necessarily global optimum.
"""
choice: """
Correct
Incorrect
"""
points: "10"
answer: "A"


@ Problem - radio

title: "Question 12 - Text Clustering"
content: """
We've shown that we can run EM for unsupervised learning of Naive Bayes in class. Which step is doing the 'soft version' of the supervised Naive Bayes algorithm?
"""
choice: """
The E step
The M step
Both of them
Neither of them
"""
points: "10"
answer: "B"