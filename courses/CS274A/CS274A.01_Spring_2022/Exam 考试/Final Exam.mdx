«««
code: CS274A
name: Natural Language Processing
semester: Spring 2022
category: Exam 考试
title: Final Exam
»»»

# Final Exam

## Page 1 (question)

@ Reading

- 所有题目随机排序。
- 所有题目为单项选择和多项选择。多项选择至少有一个正确答案，漏选按百分比扣分，错选不得分。
- 计时器设为90分钟，到时间会自动提交。
- 按学校要求，对于试题疑问不作答疑。

@ Problem - checkbox

title: "Question 1 - Attention 1"
content: """
Select one or more correct statements
"""
choice: """
The attention scores are calculated by 'keys' and 'values'.
The attention scores are calculated by 'queries' and 'values'.
The attention scores are calculated by 'queries' and 'keys'.
The final attention output is the weighted sum of 'keys'
The final attention output is the weighted sum of 'queries'
"""
points: "10"
answer: "C"


@ Problem - checkbox

title: "Question 2 - Attention 2"
content: """
Select all correct statements about the **RNN language model with attention** covered in the lecture. Suppose we are training the language model on a sentence of length L, and we are at time step t and predicting the (t+1)-th token.
"""
choice: """
Query vector is the hidden state of the (t+1)-th token
Query vector is the hidden state of the t-th token
Keys and values are hidden states from time step 1 to t-1.
Keys and values are hidden states from time step 1 to t.
Keys and values are hidden states from time step 1 to L.
"""
points: "10"
answer: "BD"


@ Problem - radio

title: "Question 3 - Attention 3"
content: """
Given **n** **d**-dimension vectors as queries, keys, and values, the time complexity of computing the dot-product attention output of all queries is:
"""
choice: """
$\Theta(nd^2)$
$\Theta(n^2d)$
$\Theta(nd)$
$\Theta(n^2d^2)$
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 4 - Bert and ELMo"
content: """
Which of the following statements is/are correct?
"""
choice: """
BERT is pretrained by predicting the next word given previous words.
ELMo learns task-specific combinations of BiLSTM representations in different layers.
BERT replaces 15% of tokens with [MASK] and predicts them.
Finetuning a BERT on a specific task is usually much computationally easier than pretraining it.
"""
points: "10"
answer: "BD"


@ Problem - checkbox

title: "Question 5 - Constituency Parsing 1"
content: """
Assume we have the following context-free grammar G in Chomsky normal form:

<img src="https://s1.ax1x.com/2022/06/13/XRToWR.jpg" width="50%"/>

Which of the following trees for the sentence "workers dumped sacks of garbage and junk into a bin" are correct according to G?
"""
choice: """
<img src='https://s1.ax1x.com/2022/06/13/XR7Klq.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7QXV.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7YtJ.jpg' width='50%'/>
<img src='https://s1.ax1x.com/2022/06/13/XR7BnK.jpg' width='50%'/>
"""
points: "10"
answer: "AC"


@ Problem - checkbox

title: "Question 6 - Constituency Parsing 2"
content: """
Which of the following is/are true about the CKY algorithm?
"""
choice: """
CKY is a top-down parsing algorithm
It requires the grammar be in Chomskey Normal Form (CNF)
CKY runs in cubic time with respect to the sentence length
CKY can be converted to the inside algorithm by replacing <b>max</b> with <b>sum</b>.
"""
points: "10"
answer: "BCD"


@ Problem - checkbox

title: "Question 7 - Constituency Parsing 3"
content: """
Consider the language defined by the following grammar.

<img src="https://s1.ax1x.com/2022/06/13/XRHsK0.md.jpg" width="50%"/>

Select all sentences that are generated by the grammar.
"""
choice: """
oslo in snow adores kim
kim adores snow in oslo
snow adores in oslo kim
in kim adores oslo snow
"""
points: "10"
answer: "AB"


@ Problem - radio

title: "Question 8 - Constituency Parsing 4"
content: """
Consider the sentence "I like to run" and a PCFG with nonterminals {S, VP, N, P, V}, terminals {I, like, to, run}, start symbol S, and rules

<center><img src="https://s1.ax1x.com/2022/06/13/XRbPZ8.jpg" width="20%"/></center>

<center><img src="https://s1.ax1x.com/2022/06/13/XRbVRs.jpg" width="30%"/></center>

The figure is a CKY chart, with possible symbols for the leaves already filled in. We want to fill the chart with nonterminals with log probability greater than negative infinity (i.e., nonterminals that are possible to build over these spans of the sentence). Which other cells contain at least one nonterminal with log probability greater than negative infinity?
"""
choice: """
1, 3, 6
1, 3, 4, 6
1, 2, 3, 4, 6
1, 3, 4, 5, 6
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 9 - Constituency Parsing 5"
content: """
Consider the following grammars. They all share the same lexicon.

<img src="https://s1.ax1x.com/2022/06/13/XRbqO0.md.jpg" width="50%"/>

Which of these grammars is in Chomsky Normal Form?
"""
choice: """
Grammar 1
Grammar 2
Grammar 3
None of the above.
"""
points: "10"
answer: "B"


@ Problem - radio

title: "Question 10 - Dependency Parsing 1"
content: """
Consider the sentence: I am feeling confident .

<img src="https://s1.ax1x.com/2022/06/13/XRqvut.png" width="50%"/>

Given the arc scores above, what's the dependent of ROOT if we run the Eisner Algorithm?
"""
choice: """
I
am
feeling
confident
.
"""
points: "10"
answer: "D"


@ Problem - radio

title: "Question 11 - Dependency Parsing 2"
content: """
What can we get if we substitute `sum` for `max` (将`max`替换为`sum`) in the Eisner Algorithm?
"""
choice: """
The score of the projective dependency parse tree with the highest score.
The sum of the scores of all the projective dependency parse trees.
The product of the scores of all the projective dependency parse trees.
The result has no meaning in practice.
"""
points: "10"
answer: "B"


@ Problem - radio

title: "Question 12 - Dependency Parsing 3"
content: """
Recap the example for Chu-Liu-Edmonds Algorithm in class:

<img src="https://s1.ax1x.com/2022/06/13/XRLUUO.png" width="680px"/>

We first contract `V1` and `V2` into `V4`:

<img src="https://s1.ax1x.com/2022/06/13/XRLoMn.png" width="680px"/>

Then we contract `V4` and `V3` into `V5`:

<img src="https://s1.ax1x.com/2022/06/13/XRLXiF.png" width="680px"/>

After contraction, we have:

<img src="https://s1.ax1x.com/2022/06/13/XRLjG4.png" width="680px"/>

In class, we select edge `a` as the `bestInEdge` of `V5`.

<img src="https://s1.ax1x.com/2022/06/13/XRLxz9.png" width="680px"/>

We finally get

<img src="https://s1.ax1x.com/2022/06/13/XROZzd.png" width="680px"/>

What if we select edge `c` as the `bestInEdge` of `V5` instead? Please choose the expanded dependency parse tree.
"""
choice: """
<img src='https://s1.ax1x.com/2022/06/03/XNgYHx.png' width='150px'/>
<img src='https://s1.ax1x.com/2022/06/03/XNgUUK.png' width='150px'/>
<img src='https://s1.ax1x.com/2022/06/03/XNgBgH.png' width='150px'/>
<img src='https://s1.ax1x.com/2022/06/03/XNg7bq.png' width='150px'/>
"""
points: "10"
answer: "B"


@ Problem - radio

title: "Question 13 - Dependency Parsing 4"
content: """
Consider the following dependency parse tree. How many possible action sequences in arc-standard parsing can it be obtained from? For example, `SHIFT SHIFT SHIFT RIGHT-ARC RIGHT-ARC RIGHT-ARC` is a possible action sequence to obtain `I -> love -> NLP` and `ROOT -> I`. Assume we only have actions `SHIFT`, `LEFT-ARC`, `RIGHT-ARC`.

<center><img src="https://s1.ax1x.com/2022/06/13/XRXthD.png" width="200px"/></center>
"""
choice: """
1
2
3
4
"""
points: "10"
answer: "B"


@ Problem - radio

title: "Question 14 - Dependency Parsing 5"
content: """
In the Chu-Liu-Edmonds algorithm, sometimes we would subtract a constant value `c` from the weights of all edges coming out of ROOT. Why do we do this?
"""
choice: """
We want to constrain that the ROOT has only one dependent. 
We want to constrain that each word (except ROOT) has only one head. 
We want to make the dependency tree look prettier in the paper. 
We do not have a clear purpose, just do it for fun.
"""
points: "10"
answer: "A"


@ Problem - radio

title: "Question 15 - Discourse Analysis 1"
content: """
In mention ranking, we predict an antecedent for each mention in the text (represented by solid lines). Which of the following coreference links represented by the dashed lines can be inferred according to the transitive closure?

<center><img src="https://s1.ax1x.com/2022/06/13/XRX4un.png" width="40%"/></center>
"""
choice: """
A
B
Both of them
Neither of them
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 16 - Discourse Analysis 2"
content: """
Consider the sentence:

"My sister borrowed her book to me." Alice explained.

There are 6 mentions in this sentence: My, My sister, her, her book, me, Alice

The gold clustering is:

1 - My, me, Alice

2 - My sister, her

3 - her book

The predicted clustering is:

1 - My sister, her, Alice

2 - My, her book

3 - me

Under the B-cubed metric, what is the precision?
"""
choice: """
2/3
5/9
11/18
13/36
None of the above
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 17 - Language Modeling 1"
content: """
Karen builds a language model with vocabulary size of 10000. She finds a paper that also proposes a language model. The paper reports their result on the PTB dataset with vocabulary size of 10000, saying that their perplexity is 113.78. Karen tests her model on the same dataset and finds the perplexity of her model is 119.54. What can we conclude?
"""
choice: """
Karen's model is worse than the model proposed in the paper. Her perplexity is higher than the one reported in the paper. Since the vocabulary size is exactly the same, we can safely compare these two models by perplexity. 
Karen's model is better than the model proposed in the paper. Her perplexity is higher than the one reported in the paper. Since the vocabulary size is exactly the same, we can safely compare these two models by perplexity. 
We cannot tell which language model is better since the vocabulary might be different. If Karen confirms that the vocabulary is exactly the same (i.e., the same set of tokens), then her model is worse than the model proposed in the paper. 
We cannot tell which language model is better since the vocabulary might be different. If Karen confirms that the vocabulary is exactly the same (i.e., the same set of tokens), then her model is better than the model proposed in the paper.
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 18 - Language Modeling 2"
content: """
Recall the lecture slides when we use a language model to predict the next word in a sentence:

<center><img src="https://s1.ax1x.com/2022/06/13/XRjRIK.jpg" width="50%"/></center>

Which of the following is true?
"""
choice: """
This is a unigram model.
This is a bigram model.
This is a trigram model.
This is a 4-gram model.
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 19 - Language Modeling 3"
content: """
In class we mentioned that we can use gradient clipping to deal with exploding gradient. Can we use gradient amplification to deal with vanishing gradient (i.e., if we encounter vanishing gradient, we amplify the gradient by multiplying a constant)? Why?
"""
choice: """
Yes. The vanishing gradient is most likely caused by a long derivation path. Amplification would compensate for the gradient loss along the path. 
Yes. Amplifying the gradient would help us jump out of the local minimum and achieve better results. 
No. In most cases, we cannot tell whether the model gets into the vanishing gradient problem or the model just converges. 
No. It is likely that the vanishing gradient wants the parameter to stay still and the direction of the gradient is not necessarily correct. Amplification would lead the model in the wrong direction.
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 20 - Lexical Semantics"
content: """
Choose the **incorrect** statement about lexical semantics.
"""
choice: """
A word has one representation in stastic word representations (e.g., Word2Vec), a finite number of representations in formal lexical semantics, and (theoretically) an infinite number of representations in contextual word representations (e.g., BERT).
Word embeddings are dense vector representations and can be learnt automatically. Word senses are discrete and created by human.
The relationship between 'cat' and 'animal' is is-a (or superset if inversing the pair).
We cannot use a CRF sequence labeling model for word sense disambiguation like the one for POS tagging because the number of senses for different words varies.
"""
points: "10"
answer: "D"


@ Problem - checkbox

title: "Question 21 - Sentence Semantics 1"
content: """
Choose all correct statements about formal meaning representation.
"""
choice: """
Canonical form says one meaning should have exactly one representation. This indicates that the mapping between natural language and formal meaning representation is a N-to-1 mapping.
We can do inference or execution with formal meaning representations (given symbolic knowledge bases) to draw new conclusions.
Programming languages like Python cannot be formal meaning representations because they cannot express all natural language meaning.
We can convert semantic graphs to equivalent sequence representations (like first-order logic).
"""
points: "10"
answer: "BD"


@ Problem - checkbox

title: "Question 22 - Sentence Semantics 3"
content: """
Choose all correct statements about semantic role labeling.
"""
choice: """
Semantic role labeling only identifies predicate-argument structures. It may not identify relationships between adjectives and nouns.
PropBank and FrameNet are two widely used semantic role specifications. Generally speaking, roles in PropBank are more specific while roles in FrameNet are more general.
One sentence may contain multiple predicates. For one predicate, its arguments cannot cross each other, so we can predict them with sequence labeling.
Semantic role labeling and joint extraction in information extraction can be solved by similar methods because they both predict spans and relations.
"""
points: "10"
answer: "ACD"


@ Problem - checkbox

title: "Question 23 - Seq2seq 1"
content: """
Select all correct statements about seq2seq.
"""
choice: """
Lots of tasks can be formulated as a sequence-to-sequence problem, such as parsing and named entity recognition.
Training a seq2seq model requires paired input-output sequences (parallel corpus)
Learning a seq2seq model is learning a conditional language model
Neural seq2seq model without cross-attention between encoder and decoder has a severe bottleneck problem
We can use beam search to find the optimal solution in seq2seq.
"""
points: "10"
answer: "ABCD"


@ Problem - checkbox

title: "Question 24 - Seq2seq 2"
content: """
Which of the following statements about seq2seq transformer is/are correct?
"""
choice: """
To prevent seeing the future during training, we mask the self-attention layers in the transformer decoder.
To prevent seeing the future during training, we mask the self-attention layers in the transformer encoder.
To prevent seeing the future during training, we also mask the cross-attention layers.
In the cross-attention layers of the seq2seq transformer, queries and keys are hidden states from the transformer encoder., and values are hidden states from the transformer decoder.
In the cross-attention layers of the seq2seq transformer, keys and values are hidden states from the transformer decoder., and queries are hidden states from the transformer encoder.
"""
points: "10"
answer: "A"


@ Problem - radio

title: "Question 25 - Sequence Labeling 1"
content: """
The Forward-Backward algorithm for hidden Markov models (HMM) can be used to compute:
"""
choice: """
The probability of the most likely set of labels for a given sequence of observations
The probability of a sequence of observations for a given sequence of corresponding labels
The marginal probability of a sequence of observations
The joint probability of a sequence of observations and, for every observation in the sequence, every possible corresponding label
"""
points: "10"
answer: "C"


@ Problem - radio

title: "Question 26 - Sequence Labeling 2"
content: """
A hidden Markov model is a (1), which means that the model specifies (2). (Select the response that correctly completes the two missing pieces.)
"""
choice: """
(1) discriminative model; (2) P(X, Y)
(1) discriminative model; (2) P(Y | X)
(1) generative model; (2) P(X, Y)
(1) generative model; (2) P(Y | X)
"""
points: "10"
answer: "C"


@ Problem - checkbox

title: "Question 27 - Sequence Labeling 3"
content: """
Select all correct statements regarding the Baum-Welch algorithm
"""
choice: """
It is used for supervised learning of HMMs.
It converges to a global optimum after a sufficient number of iterations
In the M-step of the Baum-Welch algorithm, the forward-backward algorithm is needed to compute the expected counts. In the E-step, the 'count and normalize' strategy is used to update parameters. 
The Baum-Welch algorithm is a special case of the EM algorithm.
"""
points: "10"
answer: "D"


@ Problem - checkbox

title: "Question 28 - Sequence Labeling 4"
content: """
Select all correct statements regarding HMM, MEMM, CRF
"""
choice: """
MEMM suffers from the label bias issue
MEMM only considers the relations between adjacent labels. It does not consider contextual information
MEMM is a globally normalized model. 
HMM is a directed graphical model, while CRF is an undirected graphical model.
"""
points: "10"
answer: "AD"


@ Problem - checkbox

title: "Question 29 - Sequence Labeling 5"
content: """
Select all correct statements regarding CRF
"""
choice: """
Margin-based loss can be used in CRF learning. It involves the use of the forward algorithm.
We can maximize the conditional log-likelihood of a CRF for supervised learning. The gradient formula involves expected counts, which can be computed by either the forward-backward algorithm or using automatic differentiation to differentiate through the forward algorithm.
Neural CRFs use neural networks to compute potential functions. To learn neural CRFs, we can use similar objective functions and optimization methods to those in traditional feature-based CRF learning. 
We cannot perform unsupervised learning of CRFs in a similar way to unsupervised learning of HMMs, because it is impossible to compute the marginal log-likelihood of a given sentence. Fortunately, we can use CRF auto-encoder (CRFAE) for unsupervised learning.
"""
points: "10"
answer: "BCD"


@ Problem - checkbox

title: "Question 30 - Text Classification 1"
content: """
Choose all correct statements
"""
choice: """
The training set of text classification has labels, while the training set of text clustering does not.
Logistic regression finds the class $c$ for text $d$ by modeling the prior and the likelihood $argmax_{c \in C} p(d|c) p(c)$
Precision is the percentage of positve items that were correctly identified by the system: true positives / (true positives + false negatives)
Macro-averaging scores are the average of scores per class.
"""
points: "10"
answer: "AD"


@ Problem - checkbox

title: "Question 31 - Text Classification 2"
content: """
Choose all correct statements.
"""
choice: """
Naive Bayes is a unigram language model conditioned on the classes.
The condition independence in Naive Bayes assumes that the distribution of generating a word $x_i$ are independent of its position $i$. That is $\forall ij, p(x_i|c)=p(x_j|c)$.
For any logistic regression model with weight matrix $\mathbf{w} \in \mathbb{R}^{K \times V}$, an equivalent classifier can be constructed using $(K-1) \times V$ weights, where $K$ is the number of classes and $V$ is the dimension of feature vectors.
When classifying an example x, a naive Bayes classifier predicts true (T) instead of false (F) if the following quantity is positive: $\sum_{i=1}^n \log \frac{p(x_i|y=T)}{p(x_i|y=F)} + \log \frac{p(y=T)}{P(y=F)}$
"""
points: "10"
answer: "ACD"


@ Problem - radio

title: "Question 32 - Text Classification 3"
content: """
Given the following training set of short movie reviews, each labeled with a genre, either comedy or action:

| 	|Document|	Class|
|:--:|:--:|:--:|
|1. |	fun, couple, love, love	|comedy|
|2. |	fast, furious, shoot	|action|
|3. |	couple, fly, fast, fun, fun	|comedy|
|4. |	fly, fast, shoot, love	|action|

and a new document D:

           fast, couple, shoot, fly

compute the most likely class for D for two cases:

(1) Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.

(2) Assume a naive Bayes classifier and use add-10 smoothing for the likelihoods.
"""
choice: """
(1) action (2) action
(1) action (2) comedy
(1) comedy (2) action
(1) comedy (2) comedy 
"""
points: "10"
answer: "A"


@ Problem - radio

title: "Question 33 - Text Clustering"
content: """
We've shown that we can run EM for unsupervised learning of Naive Bayes in class. Which step is doing the 'soft version' of the supervised Naive Bayes algorithm?
"""
choice: """
The E step
The M step
Both of them
Neither of them
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 34 - Text Normalization"
content: """
Choose all correct statements.
"""
choice: """
Stemming: the stem of a word must be a substring of it. 
Lemmatization: the lemma of a word is the dictionary headword form.
Word normalization can ease the data sparseness problem, but may cause ambiguity. 
When the vocabulary size of a BPE tokenizer is unlimited, the tokenizer will always produce the same segmentation as a white-space based tokenizer on the test set.
"""
points: "10"
answer: "BC"


@ Problem - radio

title: "Question 35 - Transformer Encoder 1"
content: """
For the following components of the transformer encoder:

A. Multi-head self-attention layer.

B. Position embedding.

C. Feed forward layer.

D. Layer normalization.

E. Residual connection.

Which component does each of the following statements describe?

- 1. Reduce uninformative variation to make training more stable. [ __ ]

- 2. Building block of transformer that enables long-distance interaction. [ __ ]

- 3. Introduce non-linearity to improve model ability. [ __ ]

- 4. Encode positional information. [ __ ]

- 5. Directly passing 'raw' embeddings to the next layer [ __ ].

The right order (from 1 to 5) is:
"""
choice: """
DBCAE
CBEAD
DACBE
CEDAB
DACEB
EACBA
"""
points: "10"
answer: "C"


@ Problem - checkbox

title: "Question 36 - Word Representation 1"
content: """
Choose all of the word vector models that produce **static** and **dense** word embeddings.
"""
choice: """
One-hot vectors
PPMI vectors (without SVD)
BERT
Word2vec skip-grams
Latent Semantic Analysis (LSA)
"""
points: "10"
answer: "DE"


@ Problem - checkbox

title: "Question 37 - Word Representation 2"
content: """
Select one or more correct statements.
"""
choice: """
Dense SVD vectors often work better than sparse vectors because they eliminate low-order dimensions that represent unimportant information.
Applying Add-k smoothing to PMI calculation will give frequent words slightly higher probabilities.
Tf-idf and PMI both reduce the effect of overly frequent words like “the”, “it”, or “they” that are not very informative about the context.
In word2vec, a larger window size is better because the model sees more context.
"""
points: "10"
answer: "AC"


@ Problem - radio

title: "Question 38 - Word Representation 3"
content: """
Suppose we have a super tiny corpus “I love NLP” (yes, the corpus contains only 3 words). When we use the word2vec skip-gram model, which formula correctly calculates the likelihood of the corpus?

Note: the window size is 1. When you do not have a left context (t = 1), you only need to include the probability of predicting the right context word; and it’s similar when you do not have a right context (t = 3).
"""
choice: """
P(love|I) x P(NLP|I) x  P(I|love) x P(NLP|love) x P(love|NLP)  x P(I|NLP)
P(love|I) x P(I|love) x P(NLP|love) x P(love|NLP)
P(I) x P(love|I) x P(NLP|I, love)
P(love|I) x P(NLP|I) x  P(love|NLP) x P(l|NLP)
P(I) x P(love|I) x P(NLP|love)
"""
points: "10"
answer: "B"


@ Problem - checkbox

title: "Question 39 - Sentence Semantics 2"
content: """
Choose the **incorrect** statement(s) about lambda calculus.
"""
choice: """
A lambda term is a function from values to formulas.
The lambda notation in semantics parsing is used to represent incomplete/partial semantics.
In the syntax-driven semantic parsing model, semantic attachments of leaf nodes constrain the space of possible syntactic parse trees.
When doing the lambda reduction $[\lambda x.\lambda y. Friends(x,y)](a)$, we can substitute either $x$ or $y$ with $a$.
"""
points: "10"
answer: "CD"


@ Problem - checkbox

title: "Question 40 - Information Extraction"
content: """
Choose all correct statements
"""
choice: """
Sequence labeling models are typically not used to predict nested entities because of ambiguous labeling.
'NER as span classification' predicts the type of each span independently of other spans, while this is not the case in 'NER as constituency parsing'.
Consider a joint extraction model without pruning, which scores every triple containing two entities and one relation. There are $O(n^5)$ combinations for an input sentence of length $n$.
An 'entity->relation' pipeline model cannot constrain relation types according to entity types.
"""
points: "10"
answer: "AB"